import pandas as pd
import torch
from transformers import BertTokenizer, BertModel
from torch import nn

# í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ë¼ë²¨ë§µ (ì˜ˆ: ë‚˜ì´, ì£¼ì†Œ, ì£„ëª… ë“±)
label_map = {"ì£„ëª…": 0, "íŒê²°": 1, "ë“±ë“±": 2, "ë“±ë“±": 3}
num_classes = len(label_map)

# ëª¨ë¸ ì •ì˜ (í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ)
class BERTClassifier(nn.Module):
    def __init__(self, bert, hidden_size=768, num_classes=4):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.classifier(pooled_output)

# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
bertmodel = BertModel.from_pretrained("monologg/kobert")
model = BERTClassifier(bertmodel, num_classes=num_classes)
model.load_state_dict(torch.load("kobert_newlabels_finetuned.pt", map_location="cpu"))
model.eval()

# ì˜ˆì¸¡ í•¨ìˆ˜
def predict_label(text, target_label="ì£¼ì†Œ"):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=32)
    with torch.no_grad():
        logits = model(inputs['input_ids'], inputs['attention_mask'])
        probs = torch.softmax(logits, dim=1)
        return probs[0][label_map[target_label]].item()

# CSV ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("judgement.csv", encoding='cp949')

# ì»¬ëŸ¼ë³„ í‰ê·  í™•ë¥  ê³„ì‚°
target = "ì£¼ì†Œ"  # ì—¬ê¸° ë°”ê¾¸ë©´ ë¨ ("ì„±ëª…", "ë‚˜ì´", "ì£„ëª…" ë“±)
results = []
for col in df.columns:
    values = df[col].dropna().astype(str).tolist()[:100]
    probs = [predict_label(val, target_label=target) for val in values]
    avg_prob = sum(probs) / len(probs) if probs else 0
    results.append((col, avg_prob))

# ì¶œë ¥
results.sort(key=lambda x: x[1], reverse=True)
print(f"\nğŸ” CSVì—ì„œ '{target}' ì»¬ëŸ¼ìœ¼ë¡œ ê°€ì¥ ìœ ë ¥í•œ í›„ë³´:")
for col, score in results:
    print(f" - {col}: {target}ì¼ í™•ë¥  í‰ê·  {score:.4f}")
